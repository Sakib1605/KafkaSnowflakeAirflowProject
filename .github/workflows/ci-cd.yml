# ================================================================
#               CI/CD PIPELINE FOR STREAMING ANALYTICS PROJECT
# ================================================================
# This workflow automates:
# 1. Python validation for Kafka + MinIO + Airflow scripts
# 2. dbt SQL compilation and testing (CI validation)
# 3. Packaging DAGs as downloadable artifacts (local Airflow use)
# 4. dbt Production deployment (Gold Layer) into Snowflake
# 5. Final pipeline completion notification
#
# Airflow is running LOCALLY — so no remote DAG deployment is needed.
# Instead, DAGs are packaged and saved as a GitHub artifact.
# ================================================================

name: CI-CD Pipeline for Streaming Analytics Project

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  DBT_PROFILES_DIR: ./uber_dbt

jobs:

  # =============================================================
  # 1️⃣ PYTHON LINTING — Ensures Code Quality Before Anything Else
  # =============================================================
  lint_python:
    name: Validate Python Code
    runs-on: ubuntu-latest

    steps:
      # Pull latest repository code
      - name: Checkout repository
        uses: actions/checkout@v3

      # Install Python runtime
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      # Install Python dependencies for validation
      - name: Install dependencies
        run: pip install -r docker/requirements.txt || true

      # Flake8 — catches syntax issues and formatting problems
      - name: Run Flake8 Linting
        run: |
          pip install flake8
          flake8 simulation storage docker/dags || true



  # =============================================================
  # 2️⃣ DBT TESTING — Ensures SQL Models Compile & Tests Pass
  # =============================================================
  test_dbt:
    name: Test dbt Models
    runs-on: ubuntu-latest
    needs: lint_python

    steps:
      - uses: actions/checkout@v3

      # Install dbt Snowflake adapter
      - name: Install dbt
        run: pip install dbt-snowflake

      # Create profiles.yml using GitHub Secrets
      - name: Configure dbt profiles.yml
        run: |
          mkdir -p ~/.dbt
          cat <<EOF > ~/.dbt/profiles.yml
          snowflake_profile:
            target: prod
            outputs:
              prod:
                type: snowflake
                account: ${{ secrets.SNOWFLAKE_ACCOUNT }}
                user: ${{ secrets.SNOWFLAKE_USER }}
                password: ${{ secrets.SNOWFLAKE_PASSWORD }}
                role: ${{ secrets.SNOWFLAKE_ROLE }}
                warehouse: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
                database: ${{ secrets.SNOWFLAKE_DATABASE }}
                schema: TRANSFORM
          EOF

      # Validate SQL — ensures dbt ref(), sources, macros resolve
      - name: Run dbt compile
        working-directory: uber_dbt
        run: dbt compile

      # Run dbt schema/data tests
      - name: Run dbt tests
        working-directory: uber_dbt
        run: dbt test



  # =============================================================
  # 3️⃣ DOCKER BUILD & PUSH — Publish Airflow Environment Image
  # =============================================================
  docker_build:
    name: Build & Push Docker Images
    runs-on: ubuntu-latest
    needs: test_dbt

    steps:
      - uses: actions/checkout@v3


      # Build Airflow image defined in docker/
      - name: Build Airflow Image
        run: |
          docker build -t ${{ secrets.DOCKER_USERNAME }}/uber-airflow:latest docker/

      # Push image to registry
      - name: Push to Docker Hub
        run: docker push ${{ secrets.DOCKER_USERNAME }}/uber-airflow:latest



  # =============================================================
  # 4️⃣ PACKAGE AIRFLOW DAGS — Used For Local Deployment
  # =============================================================
  package_dags:
    name: Package DAGs for Local Airflow Use
    runs-on: ubuntu-latest
    needs: docker_build

    steps:
      - uses: actions/checkout@v3

      # Save DAG folder as a downloadable artifact (zip)
      - name: Upload DAG Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: airflow-dags
          path: docker/dags/
      # You download this and place into your local /opt/airflow/dags folder.



  # =============================================================
  # 5️⃣ DBT DEPLOYMENT — Build Gold Layer in Snowflake
  # =============================================================
  deploy_dbt:
    name: Deploy dbt to Snowflake (Prod)
    runs-on: ubuntu-latest
    needs: package_dags

    steps:
      - uses: actions/checkout@v3

      - name: Install dbt
        run: pip install dbt-snowflake

      # Recreate dbt profile for production run
      - name: Configure dbt profiles.yml
        run: |
          mkdir -p ~/.dbt
          cat <<EOF > ~/.dbt/profiles.yml
          snowflake_profile:
            target: prod
            outputs:
              prod:
                type: snowflake
                account: ${{ secrets.SNOWFLAKE_ACCOUNT }}
                user: ${{ secrets.SNOWFLAKE_USER }}
                password: ${{ secrets.SNOWFLAKE_PASSWORD }}
                role: ${{ secrets.SNOWFLAKE_ROLE }}
                warehouse: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
                database: ${{ secrets.SNOWFLAKE_DATABASE }}
                schema: TRANSFORM
          EOF

      # Build Gold Layer Models
      - name: Run dbt run
        working-directory: uber_dbt
        run: dbt run

      # Generate documentation site
      - name: Run dbt docs generate
        working-directory: uber_dbt
        run: dbt docs generate



  # =============================================================
  # 6️⃣ PIPELINE SUCCESS INDICATOR
  # =============================================================
  notify:
    name: Notify Completion
    runs-on: ubuntu-latest
    needs: deploy_dbt

    steps:
      - name: Print success message
        run: echo "CI/CD pipeline executed successfully!"
